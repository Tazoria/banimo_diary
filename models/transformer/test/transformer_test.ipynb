{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from models.transformer.Preprocess import Preprocessor\n",
    "from models.transformer import transformer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def evaluate(sentence, output):\n",
    "  # 입력 문장에 대한 전처리\n",
    "  # 디코더의 예측 시작\n",
    "  for i in range(preprocessor.MAX_LENGTH):\n",
    "    predictions = model(inputs=[sentence, output], training=False)\n",
    "\n",
    "\n",
    "    # 현재 시점의 예측 단어를 받아온다.\n",
    "    predictions = predictions[:, -1:, :]\n",
    "    print('=====predictions=====\\n', predictions)\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
    "    if tf.equal(predicted_id, preprocessor.END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # 현재 시점의 예측 단어를 output(출력)에 연결한다.\n",
    "    # output은 for문의 다음 루       프에서 디코더의 입력이 된다.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "  # 단어 예측이 모두 끝났다면 output을 리턴.\n",
    "  return tf.squeeze(output, axis=0)\n",
    "\n",
    "\n",
    "def predict(sentence_tokenized, output_vector):\n",
    "  prediction = evaluate(sentence_tokenized, output_vector)\n",
    "  print('=====encoded=====\\n', prediction)\n",
    "  # prediction == 디코더가 리턴한 대답에 해당하는 정수 시퀀스\n",
    "  # tokenizer.decode()를 통해 정수 시퀀스를 문자열로 디코딩.\n",
    "  predicted_sentence = preprocessor.tokenizer.decode(\n",
    "    [i for i in prediction if i < preprocessor.tokenizer.vocab_size])\n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Output: {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T11:57:29.523012900Z",
     "start_time": "2023-08-24T11:57:29.507207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reg_sentence: return(preprocessed) >  ['으악 이거 왜 이렇게 졸라 안되는거야 짜증나 죽겠누 !']\n",
      "load_tokenizer: return(self.tokenizer) >  <SubwordTextEncoder vocab_size=8247>\n",
      "tokenized_for_eval: return(tokenized_input, tokenized_output) >  self.tokenized_input[0]= tf.Tensor(\n",
      "[[8247 8227 8147 8179 8227 8140 8124 8023 8227 8148 8171 8225 8168 8167\n",
      "  8023 8227 8144 8147 8023 8227 8148 8171 8226 8151 8126 8225 8169 8131\n",
      "  8023 8227 8152 8175 8226 8148 8179 8023 8227 8140 8127 8226 8135 8143\n",
      "  8226 8129 8139 8225 8168 8167 8227 8140 8179 8023 8227 8158 8147 8227\n",
      "  8157 8148 8226 8121 8143 8023 8227 8154 8180 8225 8169 8151 8226 8127\n",
      "  8123 8024 8248]], shape=(1, 73), dtype=int32) self.tokenized_output= tf.Tensor([[8247]], shape=(1, 1), dtype=int32)\n",
      "(1, 8249, 256)\n",
      "(1, 8249, 256)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-20.007692  -19.884024  -20.186596  ... -20.00737   -20.022373\n",
      "    -1.7427542]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor([[[-20.4706   -19.919289 -19.706457 ... -20.47122  -20.476105  -8.302856]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-21.788254  -21.027523  -20.397795  ... -21.788235  -21.785656\n",
      "    -6.6295133]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-17.551846  -17.610363  -15.842273  ... -17.551647  -17.565304\n",
      "    -1.2161236]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor([[[-19.2103   -19.372732 -19.111591 ... -19.210438 -19.212044 -13.326558]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-23.51194   -22.963034  -22.876648  ... -23.511524  -23.491562\n",
      "    -5.9541774]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-18.24928   -17.69927   -18.14043   ... -18.249105  -18.249178\n",
      "     1.4916167]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor([[[-20.438189 -20.739414 -21.175438 ... -20.437418 -20.454075  -9.054651]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====predictions=====\n",
      " tf.Tensor(\n",
      "[[[-21.257402  -19.30363   -19.449263  ... -21.25834   -21.259737\n",
      "    -7.4930058]]], shape=(1, 1, 8249), dtype=float32)\n",
      "=====encoded=====\n",
      " tf.Tensor([8247 8227 8148 8179 8227 8148 8171 8023 8227 8149], shape=(10,), dtype=int32)\n",
      "Input: ['으악 이거 왜 이렇게 졸라 안되는거야 짜증나 죽겠누!']\n",
      "Output: 일이 �\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "  vocab_path = r'D:\\banimo_diary\\models\\vocab.txt'\n",
    "  sentence = ['으악 이거 왜 이렇게 졸라 안되는거야 짜증나 죽겠누!']\n",
    "  preprocessor = Preprocessor(vocab_path, sentence)\n",
    "  sentence_tokenized, output_vector = preprocessor(vocab_path, sentence)\n",
    "\n",
    "  model = transformer.transformer(\n",
    "    vocab_size=preprocessor.tokenizer.vocab_size+2,\n",
    "    num_layers=2,\n",
    "    dff=512,\n",
    "    d_model=256,\n",
    "    num_heads=8,\n",
    "    dropout=.1)\n",
    "\n",
    "  model.load_weights(r'D:\\banimo_diary\\models\\save\\weights\\transformer_weight100.h5')\n",
    "  output = predict(sentence_tokenized, output_vector)\n",
    "  print(len(output))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-24T11:58:21.196182500Z",
     "start_time": "2023-08-24T11:58:18.051299800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
